{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.misc import imresize\n",
    "from skimage import feature\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_chunk(set_name, chunk_nr, feature_name):\n",
    "    \n",
    "    chunk = pickle.load(open(\"./features/%s_%i-%s.pkl\" % (set_name, chunk_nr, feature_name), 'rb'))\n",
    "    \n",
    "    print('load_chunk (#%i, %s): loaded %i samples' % (chunk_nr, feature_name, chunk['n_samples']))\n",
    "    \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [('diff_dep', 3), ('hog_all', 4), ('sift_dep', 3), ('sift_rgb', 3), ('hog2_all', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_nr = 0\n",
    "ref_chunk = 0 # used for feature selection\n",
    "k = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chunk (#0, diff_dep_3): loaded 20000 samples\n",
      "load_chunk (#0, hog_all_4): loaded 20000 samples\n",
      "load_chunk (#0, sift_dep_3): loaded 20000 samples\n",
      "load_chunk (#0, sift_rgb_3): loaded 20000 samples\n",
      "load_chunk (#0, hog2_all_0): loaded 20000 samples\n",
      "features:  (20000, 24736)\n",
      "labels:    (20000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "feature_sets = []\n",
    "\n",
    "for (f, i) in features:\n",
    "    feature_sets.append(load_chunk('train', chunk_nr, '%s_%i' % (f, i)))\n",
    "\n",
    "train_raw = np.concatenate(tuple([d['values'] for d in feature_sets]), axis=1)\n",
    "print('features: ', train_raw.shape)\n",
    "\n",
    "labels_raw = feature_sets[0]['label']\n",
    "weights_raw = feature_sets[0]['weight']\n",
    "print('labels:   ', labels_raw.shape)\n",
    "\n",
    "del feature_sets[:], feature_sets\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 8696)\n",
      "(20000, 2048)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if chunk_nr == ref_chunk:\n",
    "    \n",
    "    train = train_raw\n",
    "    labels = labels_raw\n",
    "    weights = weights_raw\n",
    "    \n",
    "    del train_raw, labels_raw, weights_raw\n",
    "\n",
    "    sel_variance = VarianceThreshold(0.8*0.2)\n",
    "    sel_variance.fit(train, labels)\n",
    "    train = sel_variance.transform(train)\n",
    "\n",
    "    pre_scaler = StandardScaler()\n",
    "    pre_scaler.fit(train, labels)\n",
    "    train = pre_scaler.transform(train)\n",
    "\n",
    "    print(train.shape)\n",
    "\n",
    "    sel_kbest = SelectKBest(k=k)\n",
    "    sel_kbest.fit(train, labels)\n",
    "    train = sel_kbest.transform(train)\n",
    "\n",
    "    print(train.shape)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 4096)\n",
      "(76000, 4096) (76000,) (76000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if chunk_nr != ref_chunk:\n",
    "    \n",
    "    train_raw = sel_variance.transform(train_raw)\n",
    "    train_raw = pre_scaler.transform(train_raw)\n",
    "    train_raw = sel_kbest.transform(train_raw)\n",
    "\n",
    "    print(train_raw.shape)\n",
    "    \n",
    "    train = np.concatenate((train, train_raw), axis=0)\n",
    "    labels = np.concatenate((labels, labels_raw), axis=0)\n",
    "    weights = np.concatenate((weights, weights_raw), axis=0)\n",
    "    \n",
    "    print(train.shape, labels.shape, weights.shape)\n",
    "    \n",
    "    del train_raw, labels_raw, weights_raw\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped features to ./features/train_complete_34330_2048.pkl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "complete = {\n",
    "    'values': train,\n",
    "    'weights': weights,\n",
    "    'labels': labels,\n",
    "    'sel_variance': sel_variance,\n",
    "    'pre_scaler': pre_scaler,\n",
    "    'sel_kbest': sel_kbest,\n",
    "}\n",
    "\n",
    "id = ''.join(str(i) for (f, i) in features)\n",
    "id += '_'+str(k)\n",
    "\n",
    "path = './features/train_complete_%s.pkl' % (id)\n",
    "\n",
    "pickle.dump(complete, open(path, 'wb'))\n",
    "print('dumped features to %s' % (path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76000, 2048) (76000,) (76000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "id = ''.join(str(i) for (f, i) in features)\n",
    "id += '_'+str(k)\n",
    "\n",
    "complete = pickle.load(open('./features/train_complete_%s.pkl' % (id), 'rb'))\n",
    "\n",
    "train = complete['values']\n",
    "labels = complete['labels']\n",
    "weights = complete['weights']\n",
    "\n",
    "sel_variance = complete['sel_variance']\n",
    "pre_scaler = complete['pre_scaler']\n",
    "sel_kbest = complete['sel_kbest']\n",
    "\n",
    "print(train.shape, labels.shape, weights.shape)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n",
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.847904, total=  41.9s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.861477, total=  42.6s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.859712, total=  42.3s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.844125, total=  43.2s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.837805, total=  42.9s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.858630, total=  43.5s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  4.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.850561, total=  43.7s\n",
      "[CV] max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  5.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=20, max_features=64, min_samples_leaf=1, min_samples_split=2, n_estimators=512, score=0.842548, total=  43.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  6.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "{'max_depth': 20, 'max_features': 64, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 512}\n",
      "\n",
      "Grid scores:\n",
      "Validation: 0.850 (+/-0.008), Training: 0.994  for {'max_depth': 20, 'max_features': 64, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 512, 'k': 2048}\n",
      "\n",
      "0:06:47.923081\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "parameters = {\n",
    "        'max_depth': [20],\n",
    "        'max_features': [64],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [512],\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=1, n_jobs=-1),\n",
    "        param_grid=parameters,\n",
    "        cv=8,\n",
    "        verbose=10,\n",
    "        fit_params={'sample_weight': weights}\n",
    "#       n_jobs=-1 # infeasible (RAM)\n",
    "#       refit=False\n",
    "    )\n",
    "\n",
    "clf.fit(train, labels)\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "\n",
    "means_valid = clf.cv_results_['mean_test_score']\n",
    "stds_valid = clf.cv_results_['std_test_score']\n",
    "means_train = clf.cv_results_['mean_train_score']\n",
    "\n",
    "print(\"Grid scores:\")\n",
    "for mean_valid, std_valid, mean_train, params in zip(means_valid, stds_valid, means_train, clf.cv_results_['params']):\n",
    "    params['k'] = k\n",
    "    print(\"Validation: %0.3f (+/-%0.03f), Training: %0.3f  for %r\" % (mean_valid, std_valid, mean_train, params))\n",
    "print()\n",
    "\n",
    "print(datetime.now()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chunk (#0, diff_dep_3): loaded 8190 samples\n",
      "load_chunk (#0, hog_all_4): loaded 8190 samples\n",
      "load_chunk (#0, sift_dep_3): loaded 8190 samples\n",
      "load_chunk (#0, sift_rgb_3): loaded 8190 samples\n",
      "load_chunk (#0, hog2_all_0): loaded 8190 samples\n",
      "features:  (8190, 24736)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "feature_sets = []\n",
    "\n",
    "for (f, i) in features:\n",
    "    feature_sets.append(load_chunk('test', 0, '%s_%i' % (f, i)))\n",
    "\n",
    "test = np.concatenate(tuple([d['values'] for d in feature_sets]), axis=1)\n",
    "print('features: ', test.shape)\n",
    "\n",
    "del feature_sets[:], feature_sets\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8190, 2048)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "test = sel_variance.transform(test)\n",
    "test = pre_scaler.transform(test)\n",
    "test = sel_kbest.transform(test)\n",
    "print(test.shape)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((8190, 2))\n",
    "prediction[:, 1] = clf.predict(test)\n",
    "prediction[:, 0] = range(1,8191)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('prediction.csv', prediction, delimiter=',', fmt='%i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
