{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.misc import imresize\n",
    "from skimage import feature\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_chunk(set_name, chunk_nr, feature_name):\n",
    "    \n",
    "    chunk = pickle.load(open(\"./features/%s_%i-%s.pkl\" % (set_name, chunk_nr, feature_name), 'rb'))\n",
    "    \n",
    "    print('load_chunk (%s): loaded %i samples' % (feature_name, chunk['n_samples']))\n",
    "    \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [('diff_dep', 0), ('hog_all', 2), ('sift_dep', 1), ('sift_rgb', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_nr = 3\n",
    "ref_chunk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chunk (diff_dep_0): loaded 16000 samples\n",
      "load_chunk (hog_all_2): loaded 16000 samples\n",
      "load_chunk (sift_dep_1): loaded 16000 samples\n",
      "load_chunk (sift_rgb_1): loaded 16000 samples\n",
      "features:  (16000, 16544)\n",
      "labels:    (16000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "feature_sets = []\n",
    "\n",
    "for (f, i) in features:\n",
    "    feature_sets.append(load_chunk('train', chunk_nr, '%s_%i' % (f, i)))\n",
    "\n",
    "train_raw = np.concatenate(tuple(map(lambda d: d['values'], feature_sets)), axis=1)\n",
    "print('features: ', train_raw.shape)\n",
    "\n",
    "labels_raw = feature_sets[0]['label']\n",
    "weights_raw = feature_sets[0]['weight']\n",
    "print('labels:   ', labels_raw.shape)\n",
    "\n",
    "del feature_sets[:], feature_sets\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 7076)\n",
      "(20000, 2048)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if chunk_nr == ref_chunk:\n",
    "    \n",
    "    train = train_raw\n",
    "    labels = labels_raw\n",
    "    weights = weights_raw\n",
    "    \n",
    "    del train_raw, labels_raw, weights_raw\n",
    "\n",
    "    sel_variance = VarianceThreshold(0.8*0.2)\n",
    "    sel_variance.fit(train, labels)\n",
    "    train = sel_variance.transform(train)\n",
    "\n",
    "    pre_scaler = StandardScaler()\n",
    "    pre_scaler.fit(train, labels)\n",
    "    train = pre_scaler.transform(train)\n",
    "\n",
    "    print(train.shape)\n",
    "\n",
    "    sel_kbest = SelectKBest(k=2048)\n",
    "    sel_kbest.fit(train, labels)\n",
    "    train = sel_kbest.transform(train)\n",
    "\n",
    "    print(train.shape)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2048)\n",
      "(76000, 2048) (76000,) (76000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if chunk_nr != ref_chunk:\n",
    "    \n",
    "    train_raw = sel_variance.transform(train_raw)\n",
    "    train_raw = pre_scaler.transform(train_raw)\n",
    "    train_raw = sel_kbest.transform(train_raw)\n",
    "\n",
    "    print(train_raw.shape)\n",
    "    \n",
    "    train = np.concatenate((train, train_raw), axis=0)\n",
    "    labels = np.concatenate((labels, labels_raw), axis=0)\n",
    "    weights = np.concatenate((weights, weights_raw), axis=0)\n",
    "    \n",
    "    print(train.shape, labels.shape, weights.shape)\n",
    "    \n",
    "    del train_raw, labels_raw, weights_raw\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete = {\n",
    "    'values': train,\n",
    "    'weights': weights,\n",
    "    'labels': labels,\n",
    "}\n",
    "\n",
    "pickle.dump(complete, open('./features/train_complete_0211.pkl', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76000, 2048) (76000,) (76000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "complete = pickle.load(open('./features/train_complete_0211.pkl', 'rb'))\n",
    "train = complete['values']\n",
    "labels = complete['labels']\n",
    "weights = complete['weights']\n",
    "\n",
    "print(train.shape, labels.shape, weights.shape)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512 \n",
      "[CV]  max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512, score=0.877927, total= 2.5min\n",
      "[CV] max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512, score=0.870017, total= 2.4min\n",
      "[CV] max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512, score=0.853797, total= 2.3min\n",
      "[CV] max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512, score=0.854784, total= 2.3min\n",
      "[CV] max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  9.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=16, max_features=64, min_samples_leaf=4, min_samples_split=16, n_estimators=512, score=0.863403, total= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "{'max_depth': 16, 'max_features': 64, 'min_samples_leaf': 4, 'min_samples_split': 16, 'n_estimators': 512}\n",
      "\n",
      "Grid scores:\n",
      "Validation: 0.864 (+/-0.009), Training: 0.949  for {'max_depth': 16, 'max_features': 64, 'min_samples_leaf': 4, 'min_samples_split': 16, 'n_estimators': 512}\n",
      "\n",
      "0:14:57.367553\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "parameters = {\n",
    "        'max_depth': [16],\n",
    "        'max_features': [64],\n",
    "        'min_samples_split': [16],\n",
    "        'min_samples_leaf': [4],\n",
    "        'n_estimators': [512],\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=1, n_jobs=-1),\n",
    "        param_grid=parameters,\n",
    "        cv=5,\n",
    "        verbose=10,\n",
    "        fit_params={'sample_weight': weights}\n",
    "#       n_jobs=-1 # infeasible (RAM)\n",
    "#       refit=False\n",
    "    )\n",
    "\n",
    "clf.fit(train, labels)\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "\n",
    "means_valid = clf.cv_results_['mean_test_score']\n",
    "stds_valid = clf.cv_results_['std_test_score']\n",
    "means_train = clf.cv_results_['mean_train_score']\n",
    "\n",
    "print(\"Grid scores:\")\n",
    "for mean_valid, std_valid, mean_train, params in zip(means_valid, stds_valid, means_train, clf.cv_results_['params']):\n",
    "    print(\"Validation: %0.3f (+/-%0.03f), Training: %0.3f  for %r\" % (mean_valid, std_valid, mean_train, params))\n",
    "print()\n",
    "\n",
    "print(datetime.now()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_chunk (diff_dep_0): loaded 8190 samples\n",
      "load_chunk (hog_all_2): loaded 8190 samples\n",
      "load_chunk (sift_dep_1): loaded 8190 samples\n",
      "load_chunk (sift_rgb_1): loaded 8190 samples\n",
      "features:  (8190, 16544)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "feature_sets = []\n",
    "\n",
    "for (f, i) in features:\n",
    "    feature_sets.append(load_chunk('test', 0, '%s_%i' % (f, i)))\n",
    "\n",
    "test = np.concatenate(tuple(map(lambda d: d['values'], feature_sets)), axis=1)\n",
    "print('features: ', test.shape)\n",
    "\n",
    "del feature_sets[:], feature_sets\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8190, 2048)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "test = sel_variance.transform(test)\n",
    "test = pre_scaler.transform(test)\n",
    "test = sel_kbest.transform(test)\n",
    "print(test.shape)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((8190, 2))\n",
    "prediction[:, 1] = clf.predict(test)\n",
    "prediction[:, 0] = range(1,8191)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('prediction.csv', prediction, delimiter=',', fmt='%i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
